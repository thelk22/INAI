{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This worksheet covers the three supervised learning algorithms we looked at in week 14: k-nearest neighbours, linear regression, and the naive Bayes classifier. Similar to last week, you will do some work implementing your own versions of these algorithms, to ensure that you understand the details of them. You will also compare them with the implementations in scikit-learn to test your implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "Import key packages: NumPy, matplotlib, and any others that you prefer to work with. In general, when writing code, you will put all your import statements at the top. However, for these worksheets we will import as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: import NumPy and matplotlib here\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: k-nearest neighbours classification\n",
    "In this question we will use the k-nearest neighbours algorithm to make predictions on the breast cancer Wisconsin dataset. This is a classification problem where the aim is to classify instances as either being malignant or benign based on the following 10 features:\n",
    "1. radius (mean of distances from center to points on the perimeter)\n",
    "2. texture (standard deviation of gray-scale values)\n",
    "3. perimeter\n",
    "4. area\n",
    "5. smoothness (local variation in radius lengths)\n",
    "6. compactness (perimeter squared/ area −1)\n",
    "7. concavity (severity of concave portions of the contour)\n",
    "8. concave points (number of concave portions of the contour)\n",
    "9. symmetry\n",
    "10. fractal dimension (‘coastline approximation’ −1)\n",
    "\n",
    "In this question you will (a) download the dataset from sklearn and store the data and targets in suitable variables, (b) separate your data into a training and test split, (c) write your own function to implement k-nearest neighbours, (d) check your implementation with that of sklearn. Wethen go on to (e) select the most appropriate value of $k$ using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a)\n",
    "Import the package `datasets` from `sklearn` and then load the breast cancer dataset (function is `load_breast_cancer()`). Save the data into a variable `X` and the targets into a variable `Y`. \n",
    "Take a look at the data in `X`. How many datapoints are there? How many features does each datapoint have? (Hint: use `np.shape`).\n",
    "Take a look at the targets. Is this suitable for a classification algorithm or a regression algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: import suitable packages, load the dataset, and save data and targets into variables X and Y\n",
    "from sklearn import datasets\n",
    "\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "Y = breast_cancer.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)\n",
    "\n",
    "Use the function `train_test_split` from `sklearn.model_selection` to split your data into a training set and a held-out test set. Use a test set that is 0.2 of the original dataset. Set the parameter `random_state` to 10 to help with replication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import the package train_test_split from sklearn.model_selection.\n",
    "# Split the dataset into Xtr, Xtest, Ytr, Ytest. Xtest and Ytest will form your held-out\n",
    "# test set. You will later split Xtr and Ytr into training and validation sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtr, Xtest, Ytr, Ytest = train_test_split(X, Y, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c) \n",
    "Recall from the lecture that the k-nearest neighbours algorithm runs as follows:\n",
    "\n",
    "Training step: Simply store the dataset\n",
    "\n",
    "Prediction step: Given a datapoint $\\vec{x}$:\n",
    " - **Find** the k datapoints $(\\vec{x}_i, y_i)$ where the distance from $\\vec{x}$ to $\\vec{x}_i$ is smallest\n",
    " - **Return** the majority class from the $y_i$\n",
    "   \n",
    " \n",
    "What, if anything, do you need to do for the training step?\n",
    "\n",
    "Write function(s) to implement the k-nearest neighbours prediction step. You may wish to break the procedure down into two functions `predict_datapoint` that makes a prediction for one datapoint and `predict_data` that loops over the whole dataset.\n",
    "\n",
    "To select the majority class from the nearest neighbours, you can use the function `scipy.stats.mode()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "##TODO##: \n",
    "# Write function(s) to implement the prediction step in k-nearest neighbours. \n",
    "# You can use the suggested structure below if desired.\n",
    "\n",
    "# predict_datapoint takes 4 arguments. pt (type: numpy array) is the datapoint we are making a prediction about, \n",
    "# Xtrain and Ytrain (numpy arrays) are training data and targets, k (int) is the number of neighbours.\n",
    "# Returns an integer which is the predicted class for pt\n",
    "def predict_datapoint(pt, Xtrain, Ytrain, k):\n",
    "    \n",
    "    # For each datapoint in Xtrain, calculate the distance to pt and store\n",
    "    dists = np.linalg.norm(Xtrain-pt, axis=1)\n",
    "\n",
    "        \n",
    "    # Sort the list of distances (hint: use np.argsort)\n",
    "    indices = np.argsort(dists)\n",
    "    \n",
    "    # obtain the classes (in Ytrain) of the datapoints with the smallest distance to pt\n",
    "    classes = Ytrain[indices[:k]]\n",
    "    \n",
    "    # return the mode of the classes\n",
    "    return mode(classes).mode[0]\n",
    "\n",
    "# predict_data takes 4 arguments: the test data Xtst (numpy array), the training data Xtrain (numpy array),\n",
    "# the training targets Ytrain (numpy array), and the number of neighbours k (int, default = 3). \n",
    "# Returns: predictions (array of int) for each point in Xtst\n",
    "def predict_data(Xtst, Xtrain, Ytrain, k=3):\n",
    "    #Loop over the datapoints in Xtst and store the prediction for that datapoint\n",
    "    preds = []\n",
    "    for x in Xtst:\n",
    "        preds.append(predict_datapoint(x, Xtrain, Ytrain, k))\n",
    "        \n",
    "    # Return the predictions\n",
    "    return preds\n",
    "\n",
    "# Predict values for the TRAINING data (we will not look at the test set yet)\n",
    "my_Ypred = predict_data(Xtr, Xtr, Ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d)\n",
    "Now we can compare your implementation with the sklearn implementation (you should get the same results). Firstly import the classfifier `KNeighborsClassifier` from `sklearn.neighbors`. Instantiate the classifier with the same number of neighbours that you used previously. Fit the model and make a prediction on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO##\n",
    "# Import KNeighborClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Instantiate the classifier with 3 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "#Fit the classifier on the training data\n",
    "model=knn.fit(Xtr, Ytr)\n",
    "\n",
    "#Make a prediction on the training data\n",
    "Ypred=model.predict(Xtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether your predictions are the same as the predictions from `KNeighborsClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO## Write your answer here\n",
    "np.all(my_Ypred==Ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the built in metrics in sklearn to calculate the accuracy of your classifier on the TRAINING set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_Ypred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-55803436d271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##TODO## Write your answer here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maccuracytr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmy_Ypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracytr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_Ypred' is not defined"
     ]
    }
   ],
   "source": [
    "##TODO## Write your answer here\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracytr=accuracy_score(Ytr,my_Ypred)\n",
    "print(accuracytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part(e) Using cross-validation for model selection\n",
    "k-nearest neighbours has the parameter $k$, and we need to decide which is the best value of $k$ to use. Last week we talked about using cross-validation for model selection.\n",
    "\n",
    "We will use cross-validation on our training set to select the best value of $k$, in a range from 1 to 30.\n",
    "\n",
    "NB: use sklearn's version of k-NN rather than yours, since unless you have optimised yours it is probably too slow.\n",
    "\n",
    "Since we are using cross-validation for model selection we will cross-validate on the training set only.\n",
    "\n",
    "Procedure:\n",
    "        \n",
    " 1. Import `KFold` from `sklearn.model_selection`  \n",
    " 1. Instantiate `KFold` with 5 splits. Set the parameter `random_state` to help you reproduce your results if needed.\n",
    " 1. Set a variable `max_k` to 30  \n",
    " 1. Inititalise two variables to store the training accuracies and validation accuracies (these need to store max_k\\*5 accuracies)  \n",
    " 1. Loop over the values of k:  \n",
    "    1. Instantiate a k-nn classifier (Use the sklearn classifier) with the current value of k  \n",
    "    1. Loop over the cross-validation splits:  \n",
    "       1. fit the model on the current split of data  \n",
    "       1. make predictions  \n",
    "       1. calculate training and validation accuracy and store  \n",
    " 6. Calculate the mean training and validation accuracies across splits for each $k$\n",
    "\n",
    "Plot the mean training and validation accuracies. Which value of $k$ will you use? Why?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO##\n",
    "# Import KFold from sklearn.model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Instantiate KFold with 5 splits. \n",
    "# Set the parameter random_state to help you reproduce your results if needed.\n",
    "cv = KFold(n_splits=5,random_state=10, shuffle=True)\n",
    "\n",
    "# Set a variable max_k to 30 \n",
    "max_k = 30\n",
    "\n",
    "# Inititalise two variables to store the \n",
    "# training accuracies and validation accuracies \n",
    "# (these need to store max_k*5 accuracies) \n",
    "trainaccuracy = [[] for _ in range(max_k)]\n",
    "valaccuracy = [[] for _ in range(max_k)]\n",
    "\n",
    "# Loop over the values of k: \n",
    "for k in range(max_k):\n",
    "    \n",
    "    # Instantiate a k-nn classifier (Use the sklearn classifier) with the current value of k \n",
    "    knn = KNeighborsClassifier(n_neighbors=k+1)\n",
    "    # Loop over the cross-validation splits: \n",
    "    for train_index, val_index in cv.split(Xtr):\n",
    "        Xtrain, Xval, Ytrain, Yval = Xtr[train_index],\\\n",
    "            Xtr[val_index], Ytr[train_index], Ytr[val_index]\n",
    "        # fit the model on the current split of data \n",
    "        model=knn.fit(Xtrain, Ytrain)\n",
    "        # make predictions \n",
    "        Ypredtrain=model.predict(Xtrain)\n",
    "        Ypredval=model.predict(Xval)\n",
    "        # calculate training and validation accuracy and store \n",
    "        trainaccuracy[k].append(accuracy_score(Ytrain,Ypredtrain))\n",
    "        valaccuracy[k].append(accuracy_score(Yval,Ypredval))\n",
    "\n",
    "# Calculate the mean training and validation accuracies across splits for each k\n",
    "mean_train_acc = np.mean(trainaccuracy, axis=1)\n",
    "mean_val_acc = np.mean(valaccuracy, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO#\n",
    "# Plot the mean training and validation accuracies. Which value of 𝑘 will you use? Why?\n",
    "x = range(1, max_k+1)\n",
    "plt.plot(x, mean_train_acc, label='Training Accuracy')\n",
    "plt.plot(x, mean_val_acc, label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The naive Bayes classifier\n",
    "\n",
    "Recall from the lecture notes that the naive Bayes classifier works as follows. We are trying to approximate an unknown function $$f:\\Omega \\rightarrow \\mathcal{O}$$\n",
    "where $\\Omega$ is our feature space and our output space $\\mathcal{O} = \\{c_1, c_2, ... c_K\\}$ is a finite set of classes.\n",
    "\n",
    "The naive Bayes classifier does this by building a model that assigns the class label $\\hat{y} = c_k$ as follows:\n",
    "$$\n",
    "\\hat{y} = \\text{argmax}_k p(c_k)\\prod_i p(x_i| c_k)\n",
    "$$\n",
    "i.e., the $k$ that maximises this quantity.\n",
    "\n",
    "In practice, multiplying all the $p(x_i| c_k)$ together is going to give some very small values. Therefore, we can take the log to make it easier to compute:\n",
    "\\begin{align}\n",
    "\\hat{y} &= \\text{argmax}_k p(c_k)\\prod_i p(x_i| c_k)= \\text{argmax}_k log(p(c_k)\\prod_i p(x_i| c_k))\\\\\n",
    "&=\\text{argmax}_k log(p(c_k)) + \\sum_i log(p(x_i|c_k))\n",
    "\\end{align}\n",
    "\n",
    "If we choose that $p(x_i|c_k)$ is given by a normal distribution with mean $\\mu_k$ and variance $\\sigma_k^2$, then we obtain the following expression:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} &=\\text{argmax}_k \\log(p(c_k)) + \\sum_i \\log(p(x_i|c_k))\\\\\n",
    "&= \\text{argmax}_k \\log(p(c_k)) + \\sum_i \\log\\left(\\frac{1}{\\sigma_k\\sqrt{2\\pi}} exp\\left(\\frac{-(x-\\mu_k)^2}{2\\sigma_k}\\right)\\right)\\\\\n",
    "&= \\text{argmax}_k \\log(p(c_k)) - \\sum_i \\log\\left(\\sigma_k\\sqrt{2\\pi}\\right) - \\sum_i\\left(\\frac{(x-\\mu_k)^2}{2\\sigma_k}\\right) \\quad \\text{ log-likelihood}\n",
    "\\end{align}\n",
    "\n",
    "Expressing the values in terms of these sums means that they do not get so small, and it is less likely that there will be errors at the machine precision level.\n",
    "\n",
    "\n",
    "How do we implement this in practice? We assume that each probability $p(x_i| c_k)$ is given by some distribution, and then given a datapoint $\\vec{x}$, we plug the value into the equation for the distribution.\n",
    "\n",
    "In this question you will (a) implement your own version of the Gaussian naive Bayes classifier, (b) check your classifier against the implementation in sci-kit learn, (c) compare the accuracy of the naive Bayes classifier with the accuracy of the k-nearest neighbours classifier, and (d) run cross-validation to verify whether the kNN classfier or the Gaussian naive Bayes classifier performs better on this dataset.\n",
    "\n",
    "## Part (a) Implementing Gaussian naive Bayes\n",
    "For this question we will make the assumption that each feature is described by a normal (also called Gaussian) distribution. The procedure is as follows:\n",
    "1. Divide the training data by class\n",
    "2. Calculate mean and standard deviation per class and per feature\n",
    "4. For each datapoint in the validation set, calculate the log-likelihood for each class and for each feature (Hint: use the function `scipy.stats.norm.logpdf`)\n",
    "5. Combine these values together with the probability of the class according to the log-likelihood equation above\n",
    "6. Choose the class with the highest value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO##\n",
    "# Write your own implementation of naive Bayes applied to the breast cancer dataset.\n",
    "\n",
    "# If you wish you can follow the structure below\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Split the training data into training and validation sets with an 80:20 split. \n",
    "# Set the random state to help with reproducibility\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(Xtr, Ytr, test_size = 0.2, random_state=10)\n",
    "\n",
    "#Separate the training set into classes, so you have one set of data for each class\n",
    "data0 = Xtrain[Ytrain==0]\n",
    "data1 = Xtrain[Ytrain==1]\n",
    "\n",
    "# Calculate the means and standard deviations for each class, for each feature. \n",
    "# There are 30 features in the dataset, so you should have a 30-dimensional \n",
    "# array of means for each class and a 30-dimensional array of standard deviations\n",
    "# for each class. Remember that you can take the average across rows or columns of \n",
    "# a matrix by specifying axis = 1 or axis = 0\n",
    "\n",
    "means0 = np.mean(data0, axis=0)\n",
    "stds0 = np.std(data0, axis=0)\n",
    "means1 = np.mean(data1, axis=0)\n",
    "stds1 = np.std(data1, axis=0)\n",
    "\n",
    "# Calculate the prior probability p(c_i) for each class\n",
    "prob_c0 = len(data0)/len(Xtrain)\n",
    "prob_c1 = len(data1)/len(Xtrain)\n",
    "\n",
    "# Calculate the log-likelihood of each class for each datapoint in the validation set\n",
    "# Hint: you can use the function scipy.stats.norm.logpdf to help with this\n",
    "ll_0 = np.log(prob_c0) + np.sum(norm.logpdf(Xval, loc=means0, scale=stds0), axis=1)\n",
    "ll_1 = np.log(prob_c1) + np.sum(norm.logpdf(Xval, loc=means1, scale=stds1), axis=1)\n",
    "\n",
    "# Your predicted class is 0 if class 0 has the highest log-likelihood, and 1 if class 1 \n",
    "# has the highest log-likelihood\n",
    "classes = ll_0 < ll_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b) Checking results\n",
    "We now compare our results with the sklearn implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import the classifier GaussianNB from sklearn.naive_bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Instantiate the classifier (use the parameter var_smoothing=0.0),\n",
    "# fit, and predict the classes\n",
    "gnb = GaussianNB(var_smoothing=0.)\n",
    "gnb.fit(Xtrain, Ytrain)\n",
    "\n",
    "val_preds = gnb.predict(Xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare your predicted classes with those of the sklearn implementation.\n",
    "# If they are not identical, this may be due to some differences in parameter setting. \n",
    "# They should be almost all the same, however.\n",
    "\n",
    "np.all(val_preds == classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c) Comparing k-nearest neighbours and Gaussian naive Bayes\n",
    "Now retrain the naive Bayes classifier using the original training set `Xtr`, `Ytr`.\n",
    "Also, retrain the k-nearest neighbours classifier using `Xtr` and `Ytr`. Use the value of $k$ that you decided on using cross-validation.\n",
    "You can use the sklearn implementations of knn and naive Bayes.\n",
    "\n",
    "Compute the accuracy of the naive Bayes classifier over the training set and the held-out test set.\n",
    "\n",
    "Compare with the accuracy of the k-nearest neighbours classifier on each set.\n",
    "\n",
    "Is it clear which classifier is the best on this dataset? why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain\t Test\n",
      "GNB\t0.9275\t0.9561\n",
      "kNN\t0.9451\t0.9298\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "##TODO## Write your answer here\n",
    "knn = KNeighborsClassifier(n_neighbors=8) #8 was the best performing value on the validation set\n",
    "\n",
    "gnb.fit(Xtr, Ytr)\n",
    "knn.fit(Xtr, Ytr)\n",
    "\n",
    "gnb_tr = gnb.predict(Xtr)\n",
    "gnb_test = gnb.predict(Xtest)\n",
    "\n",
    "knn_tr = knn.predict(Xtr)\n",
    "knn_test = knn.predict(Xtest)\n",
    "\n",
    "print(f'\\tTrain\\t Test')\n",
    "print(f'GNB\\t{accuracy_score(Ytr, gnb_tr):.4f}\\t{accuracy_score(Ytest, gnb_test):.4f}')\n",
    "print(f'kNN\\t{accuracy_score(Ytr, knn_tr):.4f}\\t{accuracy_score(Ytest, knn_test):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d) Using cross-validation for statistical validation\n",
    "Earlier we used cross-validation to select the model parameters we would be using. We can also use it another way: to provide statistical information about which model is best. We will set up cross-validation on the whole dataset, with 10 folds.\n",
    "\n",
    " - Compute the accuracy for each model on the test set on each fold.\n",
    " - Calculate the mean accuracy across folds. Which model performs best?\n",
    " - Make a box-plot of the spread of scores of each model. Is there a clear difference between model performance?\n",
    " - Perform a paired t-test on the accuracy scores. What can you conclude about the performance of the two models?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO##\n",
    "from sklearn.model_selection import KFold\n",
    "# Set up a k-fold cross-validation with 10 folds\n",
    "cv = KFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO##\n",
    "scoresknn=[]\n",
    "scoresnb=[]\n",
    "\n",
    "# For each fold, fit each model on the training data \n",
    "# and compute accuracy on the test data.\n",
    "for train_index, test_index in cv.split(X):\n",
    "    Xtr, Xtest = X[train_index], X[test_index]\n",
    "    Ytr, Ytest = Y[train_index], Y[test_index]\n",
    "    modelknn=knn.fit(Xtr, Ytr)\n",
    "    modelnb=gnb.fit(Xtr, Ytr)\n",
    "    Ypredknn=modelknn.predict(Xtest)\n",
    "    Yprednb=modelnb.predict(Xtest)\n",
    "    scoresknn.append(accuracy_score(Ytest,Ypredknn))\n",
    "    scoresnb.append(accuracy_score(Ytest,Yprednb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn: mean=0.9280388471177945, sd=0.02648458886485034\n",
      "nb: mean=0.9314536340852131,sd=0.02655961751569257\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean and standard devation of the accuracies for each model.\n",
    "# Does one model perform better?\n",
    "print(f'knn: mean={np.mean(scoresknn)}, sd={np.std(scoresknn)}')\n",
    "print(f'nb: mean={np.mean(scoresnb)},sd={np.std(scoresnb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a boxplot of the accuracy scores. Is there a clear difference between the models?\n",
    "box_plot_data=[scoresknn,scoresnb]\n",
    "plt.boxplot(box_plot_data,labels=['knn','nb'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plots show that there is no clear difference in performance between the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_relResult(statistic=-0.3374383011484767, pvalue=0.7435230898588412)\n"
     ]
    }
   ],
   "source": [
    "# Perform a paired t-test (you can use the function scipy.stats.ttest_rel). \n",
    "# What do you conclude about the performance of the two models?\n",
    "from scipy.stats import ttest_rel\n",
    "print(ttest_rel(scoresknn,scoresnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outcome of the paired t-test shows that the performance of the models are not significantly different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "In linear regression we make the assumption that the data $(x_i, y_i)$ can be modelled by a function of the form\n",
    "$$ \\hat{y_i} = f(\\vec{x}_i)= \\sum_j a_j x_{ij}  + b_i$$\n",
    "\n",
    "Recall that we can express this in a matrix format by:\n",
    "$$ \\hat{\\vec{y}} = f(X)= X\\Theta$$\n",
    "\n",
    "where \n",
    "$$ X=\\begin{pmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,n} &1 \\\\\n",
    "\\vdots & \\vdots & \\ldots & \\vdots & \\vdots \\\\\n",
    "x_{N,1} & x_{N,2} & \\ldots & x_{N,n} & 1\n",
    "\\end{pmatrix}, \\quad \\vec{y}=\\begin{pmatrix} y_1 \\\\ \\vdots \\\\y_N \\end{pmatrix}, \\quad \\Theta=\\begin{pmatrix} a_1 \\\\ \\vdots \\\\a_n\\\\b \\end{pmatrix}$$\n",
    "\n",
    "We saw in lectures that the optimal value of $\\Theta$ is given by setting\n",
    "$$ \\Theta = (X^T X)^{-1} X^T \\vec{y}$$\n",
    "\n",
    "The quantity $(X^T X)^{-1} X^T$ is called the psuedoinverse of X, and can be computed using the function `np.linalg.pinv`.\n",
    "\n",
    "We will (a) perform a linear regression on the diabetes dataset. You can load this dataset using the function `load_diabetes` from `sklearn.datasets`. (b) compute the mean squared error and the R^2, and (c) compare your results with the built in function in sklearn (`sklearn.linear_model.LinearRegresion()`). You should get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statments here\n",
    "import math\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a) Implementing linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "Y = diabetes.target\n",
    "\n",
    "# Split the dataset into training, validation and test, using test_size=0.2\n",
    "Xtr, Xtest, Ytr, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(Xtr, Ytr, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones to your Xtrain and Xtest for the intercept term\n",
    "Wtrain=np.hstack([Xtrain, np.ones((len(Xtrain),1))])\n",
    "Wval=np.hstack([Xval, np.ones((len(Xval),1))])\n",
    "Wtest=np.hstack([Xtest, np.ones((len(Xtest),1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the value of the coefficients theta. You can use the function np.linalg.pinv\n",
    "theta = np.linalg.pinv(Wtrain).dot(Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b) Computing performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction on the test set by applying the coefficients theta to the test set\n",
    "my_pred = Wval.dot(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean squared error and the R^2. \n",
    "# You can use the built in functions from sklearn\n",
    "print(f'mean squared error= {mean_squared_error(my_pred, Yval):.4f}')\n",
    "print(f'R^2={r2_score(my_pred, Yval):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c) Checking results\n",
    "Compare your results with the built in function `sklearn.linear_model.LinearRegression()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.fit(Xtrain, Ytrain)\n",
    "pred = regr.predict(Xval)\n",
    "print(f'mean squared error= {mean_squared_error(pred, Yval):.4f}')\n",
    "print(f'R^2={r2_score(pred, Yval):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the perfomance of the regression by plotting your predicted values vs target values on a scatter plot, and drawing a line y=x. If all predictions were perfect, the predicted values would lie on the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.ylim(20, 350)\n",
    "plt.xlim(20, 350)\n",
    "plt.scatter(pred,Yval,color='black')\n",
    "x = np.linspace(20,350,100) \n",
    "y=x\n",
    "plt.plot(x, y,color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra question: Polynomial regression\n",
    "The term 'linear' in linear regression refers only to the coefficients $\\theta$. We can in fact compute polynomial terms in the data and perform linear regression over this extended dataset to get a better fit to the data.\n",
    "\n",
    "To compute polynomial terms in the data automatically, you can use the class `sklearn.preprocessing.PolynomialFeatures`. To find out how to use it, look at the guidance (you can type `help(PolynomialFeatures)` once you have imported it).\n",
    "\n",
    "The following small dataset (in the cell below) gives a relationship between temperature and yield for an experiment. Use cross-validation to select the degree of the polynomial that best fits this data.\n",
    "\n",
    "Plot the mean squared error against degree on the training set and on the validation set. Which degree of polynomial best fits this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([50,50,50,70,70,70,80,80,80,90,90,90,100,100,100]).reshape(-1, 1)\n",
    "y = np.array([3.3,2.8,2.9,2.3,2.6,2.1,2.5,2.9,2.4,3,3.1,2.8,3.3,3.5,3]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "Xtr, Xtest, Ytr, Ytest = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(Xtr, Ytr, test_size=0.2, random_state=0)\n",
    "mse_tr = []\n",
    "mse_val = []\n",
    "max_deg = 10\n",
    "for i in range(max_deg):\n",
    "    poly=PolynomialFeatures(degree=i+1)\n",
    "    Xtrain_new = poly.fit_transform(Xtrain) \n",
    "    Xval_new = poly.fit_transform(Xval)\n",
    "    regr.fit(Xtrain_new, Ytrain)\n",
    "    pred_tr = regr.predict(Xtrain_new)\n",
    "    pred_v = regr.predict(Xval_new)\n",
    "    mse_tr.append(mean_squared_error(pred_tr, Ytrain))\n",
    "    mse_val.append(mean_squared_error(pred_v, Yval))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3hVVb7G8e8vBQIk9E4goUmVZgQkiAiKYAELFkZmrJfRsc14pzAzThH1jnf0Khbsol4VsCt6FURFqgIBASlK6ITQe4ck6/6xDxDCIfWc7JPk/TwPT3LO2XufH0fJm7XWXmuZcw4REZG8ovwuQEREIpMCQkREglJAiIhIUAoIEREJSgEhIiJBxfhdQKjUrVvXJScn+12GiEiZMn/+/O3OuXrBXis3AZGcnExaWprfZYiIlClmtu5Mr6mLSUREglJAiIhIUAoIEREJSgEhIiJBKSBERCQoBYSIiASlgBARkaAUECIiZdnPX8DCcWG5tAJCRKSsmvMSTPgFzH8dcrJDfvlyM5NaRKTCyMmGL/8G34+BNpfBNa9AVHTI3yasLQgzG2hmP5vZSjMbGeT1+81smZktNrOvzSwp12vZZrYw8GdiOOsUESkzjh6Ed3/lhUOPO+H6N6FS1bC8VdhaEGYWDYwBLgYygHlmNtE5tyzXYT8AKc65g2Z2J/Bv4PrAa4ecc13CVZ+ISJmzfxuMvwE2zoeBj0LPO8P6duFsQXQHVjrnVjvnjgITgCG5D3DOTXXOHQw8/B5IDGM9IiJl1/Z0eKU/bFkK178V9nCA8AZEE2BDrscZgefO5Dbgi1yP48wszcy+N7Mrg51gZiMCx6Rt27at5BWLiESitbPglYvg2EG4+f+g3eWl8rbhHKS2IM+5oAeaDQdSgAtyPd3MOZdpZi2Ab8zsR+fcqlMu5txLwEsAKSkpQa8tIlKmLX4PPvkN1EqGG9/zvpaScLYgMoCmuR4nApl5DzKzi4C/AoOdc0eOP++cywx8XQ18C3QNY60iIpHFOZj+OHx4OyR2h9u+LNVwgPAGxDygtZk1N7NKwA3AKXcjmVlX4EW8cNia6/laZlY58H1dIBXIPbgtIlJ+ZR+DiffANw9Bp+vhlx9ClVqlXkbYupicc1lmdjcwGYgGxjrnlprZKCDNOTcReAyIB94zM4D1zrnBQDvgRTPLwQuxR/Pc/SQiUj4d3gvv3QSrvoE+f4AL/woWrMc+/My58tF1n5KS4rTlqIiUaXs2wtvXwvaf4fLR0O2XYX9LM5vvnEsJ9ppmUouIRIJNi2HcdXD0gDcY3bKf3xUpIEREfJc+Bd67GeJqwq2ToEEHvysCtFifiIi/0l6DcddD7RZw+1cREw6gFoSIiD9ycuDrB2HWaGg9AIa+BpXj/a7qFAoIEZHSduwwfHwnLP0QUm6FQY9BdOT9OI68ikREyrODO2H8MNjwPVw8Cnrd69ttrAVRQIiIlJYdq7zbWPdkwLWvQ4er/K4oXwoIEZHSsGGut1S3c3DTp9Csh98VFUh3MYmIhNvSj+H1yyGuhnenUhkIB1BAiIiEj3Mw62lv6YzGXeC2r6BOS7+rKjR1MYmIhEN2FnzxR0h7FdpfCVe9CLFxfldVJAoIEZFQO7If3r8V0idD6n3Q/58QVfY6bBQQIiKhtHeTt6bSliVw2RNw7m1+V1RsCggRkVDZssy7jfXQLhj2Dpw1wO+KSqTstXlCbf9W+OQub89XEZHiWjUVxl4CLhtu/aLMhwMoIKBSPCx6B9K/9LsSESmrfngL3h4KNRK921gbdfa7opBQF1OlqtCkG6xTC0JEisg5mPpfMP3f0OJCuO4Nb65DOaEWBEBSL8j8wduoQ0SkMLKOwkd3eOHQdbi3yU85CgdQQHiSekNOljcVXkSkIJsWwSv9YPEE6PcADH4WomP9rirkFBAATbuDRcG62X5XIiKR7Nhh+HoUvHShd4PLDeOgzx8idjXWktIYBEBcdW9QSeMQInImG+Z6dzxuXwFdboRLHoEqtfyuKqwUEMclpcLcl73fEMrYdHgRCaOjB+Cbh+H75727lIZ/AK0u8ruqUqEupuOSUiH7CGyc73clIhIp1kyH53vB9895M6J/812FCQdQQJzUrCdgGocQETi8Bz69D964whufvPlzuOx/oHKC35WVKnUxHVe1NjToAOtmAn/wuxoR8cuKyfDpb2H/Zuh1D/T9izdfqgJSQOSW1MubEZl9rFzesiYi+Ti4EyaNhMXvQL12cP1bkHiO31X5Sl1MuSWlwrGDkLnQ70pEpDQt/RjGdIclH8AFf4JfT6vw4QBqQZwqqZf3dd0saHquv7WISPjt2wKf/ycs/xQadYFffgwNO/pdVcRQCyK3+PpQ9yzNhxAp75yDheO8VsOKL+Gif8LtXysc8lALIq+kXrDkQ8jJhqhov6sRkVDbvQE++y2s/Aqa9oQhz0Ld1n5XFZHUgsgrqTcc2Qubf/S7EhEJpZwcmPcKPNcT1n0Hgx6DW75QOORDLYi8ToxDzIbGXfytRURCY8cqmHivdxt7i75wxdNQK8nvqiKeWhB51WgCtZI1DiFSHuRkw+xn4PlUr1dg8DPeQLTCoVDUgggmKRV+/sJrkkYpQ0XKpK3LvcX1Ns6HNpfCZU9A9UZ+V1WmhPWnn5kNNLOfzWylmY0M8vr9ZrbMzBab2ddmlpTrtZvMLD3w56Zw1nmapFQ4tBO2/VSqbysiIZB9DKb9G144H3athWte9ZblVjgUWdhaEGYWDYwBLgYygHlmNtE5tyzXYT8AKc65g2Z2J/Bv4Hozqw38A0gBHDA/cO6ucNV7itzzIRq0L5W3FJEQyPwBPrkbtiyBjtfAoH9Dtbp+V1VmhbMF0R1Y6Zxb7Zw7CkwAhuQ+wDk31Tl3MPDweyAx8P0lwBTn3M5AKEwBBoax1lPVSobqTTQOIVJWHDsMX/0TXu4PB7bDDeNh6FiFQwmFcwyiCbAh1+MMoEc+x98GfJHPuU1CWl1+zLxWxJrp3oSacrpblEi5sP57r9WwI93bG3rAI1Clpt9VlQvhbEEE+6nqgh5oNhyvO+mxopxrZiPMLM3M0rZt21bsQoNKSoX9W7zb40Qk8hzZD1/8CcYOhKwj8MuPYMgYhUMIhTMgMoCmuR4nApl5DzKzi4C/AoOdc0eKcq5z7iXnXIpzLqVevXohKxyA5N7eV3UziUSeVVPh+fNgzgvQ/T+8jXxa9vO7qnInnAExD2htZs3NrBJwAzAx9wFm1hV4ES8ctuZ6aTIwwMxqmVktYEDgudJTpxVUq6+AEIkkmT/A+GHw5pUQXQlumQSXPgaV4/2urFwK2xiEcy7LzO7G+8EeDYx1zi01s1FAmnNuIl6XUjzwnnn9/Oudc4OdczvN7CG8kAEY5ZzbGa5agzo+DqEd5kT8lzEfpv03pE+GuJpw4V+9zXxiq/hdWbkW1olyzrnPgc/zPPf3XN+fcXNX59xYYGz4qiuEpFRY9jHsWqeZlyJ+2DAXvn0UVn0NVWpBv79B9xEQV93vyioEzaTOT3Kq93XdbAWESGla9x1MexRWfwtV63jLcZ97e4XbE9pvCoj81Gvn/daybiZ0GeZ3NSLl35oZXlfS2hlQrR4MeBhSboVK1fyurEJSQOQnKgqaaRxCJKycgzXTvOUx1s2C+AZwyb/gnJuhUlW/q6vQFBAFSeoFP/8f7N2ktVxEQsk5b2xh2r9hwxxIaOzt0dDtlxp8jhAKiIKcGIeYBWcP9bcWkfLAOUif4nUlbUyD6olw2f9A119CTGW/q5NcFBAFaXA2VEpQQIiUlHOwYpIXDJk/QI1mcPlo6HIjxFTyuzoJQgFRkOgYaNZT4xAixZWT43XTTvs3bF7sLYY5+FnofANEx/pdneRDAVEYSb3g6we9VSK1OqRI4eTkwPKJMP0xb/nt2i3gyufh7GsVDGWEAqIwTqzLNBvaD/a3FpFIl5PtTTCd9hhsWw51WsNVL3n7M0TrR05Zov9ahdGoC8RU8cYhFBAiweVkw5IPvBbD9hVQr623m1uHqyAq2u/qpBgUEIURUwmadtfCfSLBZGfBj+/BjMdhx0qo3x6ufR3aDdGe7mWcAqKwklLh23/BoV3e7GqRii77GCx+B6Y/DrvWeHf8XfcmtL1cwVBOKCAKKzkVcLB+DrQpvd1PRSJO1lFYNA5m/A/sXu91wd4wHtoM0u6L5YwCorCanOOtP79upgJCKqYj+7wWw8zRsGcDNO4Glz4OrQcoGMopBURhxVaBJimaDyEVS06Ot3DewnHeLavHDkLiud4Et1b9FQzlnAKiKJJ6wcwnvd+ktOywlGc7V8PC8bBovNdaqFwdOl0HnX/h3bChYKgQFBBFkZzq3amxYa7325NIeXJ4rzd/YeE4WP8dYN4+zxf9E9pepgX0KiAFRFEkdgeL9m53VUBIeZCT4y21vWg8LJsIWYe8iW39/wGdrocaTfyuUHykgCiKyvHQuKvGIaTs27HKayksmgB7M6ByDW9TrC43ejdkqAtJUEAUXVIvmPMCHDukJreULYf3wNKPvLGFDd+DRUHL/jDgIWhzKcTG+V2hRBgFRFEl94bZT0NGGjQ/3+9qRPKXk+11IS0cB8s/hazDULcNXPSg14WkTbAkHwqIomraAzBvHEIBIZFqe/rJLqR9mRBXA7oOhy6/8OYvqAtJCkEBUVRVakLDs7Uuk0SeQ7sDXUjjIGOu14XU6iIY+F9w1iB1IUmRKSCKIykV5r/uLTmgnbDETznZsHpqoAvpM8g+AvXawcUPefMWEhr6XaGUYQqI4khOhTnPe9smNuvhdzVSEW1b4a2HtGgC7NvkLSB5zk3QeZh3p526kCQEFBDF0ayX93XdTAWElJ5Du2DJh15rYWOaNyen9cUw6L/hrIEQU9nvCqWcUUAUR7U6XjN+7Sw4/z/9rkbKs31bYOVXsGISrJjsdSHVbw8DHvG27kxo4HeFUo4pIIorqZe3smV2lrZRlNDJzoKMebByCqRPgc2LvefjG8A5N3t3ITXqrC4kKRX6yVZcyamQ9ipsXuTNPBUprn2bvVZC+hRvwPnwHq/7qGkP6P93aHWxd+ecQkFKmQKiuJJSva/rZisgpGiys7zbUNOneC2FzT96zyc0gnZXeIHQoq93S3UFdezYMTIyMjh8+LDfpZQbcXFxJCYmEhsbW+hzFBDFldAQarf0xiF63eN3NRLp9m7yWgkrp8Cqb+FIoJXQrKe3MF7ri6FBR7USAjIyMkhISCA5ORnTZ1Jizjl27NhBRkYGzZs3L/R5CoiSSOrlbaKSk6M9eOVU2ce8ZeFXToH0r2DL8VZCY2g/2AuEFn29Gc5ymsOHDyscQsjMqFOnDtu2bSvSeQqIkkjuDT+8CVuXen3EUrHtzcw1lvAtHNkLUTHQtKe3p0Kri6FBB7USCknhEFrF+TwVECWRdHw+xGwFREWUfQw2zAmMJXwFW5Z4zyc0hg5XnhxLiKvuZ5VSDDt27KB/f2/Pl82bNxMdHU29evUAmDt3LpUqFbyCwi233MLIkSNp06bNGY8ZM2YMNWvW5MYbbwxN4SGmgCiJms2gRjNYOxN6/NrvaqQ07Nl4cixh9bSTrYRm53krpLYeAPXbqZVQxtWpU4eFCxcC8M9//pP4+Hh+//vfn3KMcw7nHFFn6F5+7bXXCnyfu+66q+TFhlFYO87NbKCZ/WxmK81sZJDX+5jZAjPLMrOheV7LNrOFgT8Tw1lniST18loQzvldiYRD1lFYMwOm/B2e6wVPtodP74WNC6DDVXD9W/DHNXDzZ9D7t9CgvcKhHFu5ciUdO3bkjjvuoFu3bmzatIkRI0aQkpJChw4dGDVq1Ilje/fuzcKFC8nKyqJmzZqMHDmSzp07c95557F161YAHnjgAUaPHn3i+JEjR9K9e3fatGnD7NnexmQHDhzgmmuuoXPnzgwbNoyUlJQT4RVuYWtBmFk0MAa4GMgA5pnZROfcslyHrQduBn5/+hU45JzrEq76QiY5FRZPgO0roN6Zm5JSRuzN9CaqZczz9vzI/MHbQyEq1rvj6OJRXteRWgml5sFPl7Isc29Ir9m+cXX+cUWHYp27bNkyXnvtNV544QUAHn30UWrXrk1WVhYXXnghQ4cOpX379qecs2fPHi644AIeffRR7r//fsaOHcvIkaf9zoxzjrlz5zJx4kRGjRrFpEmTeOaZZ2jYsCEffPABixYtolu3bsWquzjyDQgzG+6ceyvwfapzblau1+52zj2bz+ndgZXOudWB4ycAQ4ATAeGcWxt4LafYfwO/nZgPMUsBUdYcOwSZC70w2JjmBcLejd5r0ZWhcRc493av+6jFBVA5wd96JSK0bNmSc88998Tj8ePH8+qrr5KVlUVmZibLli07LSCqVKnCoEGDADjnnHOYMWNG0GtfffXVJ45Zu3YtADNnzuRPf/oTAJ07d6ZDh+IFW3EU1IK4H3gr8P0zQO7ouhXILyCaABtyPc4AirKyXZyZpQFZwKPOuY/zHmBmI4ARAM2aNSvCpUOodguIb+jNh0i51Z8apGDOwc7VXggcbyFsWQI5Wd7rtZK97sLEcyExBRqcraXcI0Rxf9MPl2rVqp34Pj09naeeeoq5c+dSs2ZNhg8fHnRyX+5B7ejoaLKysoJeu3Llyqcd43zsvi4oIOwM3wd7nN+5xxXlb9rMOZdpZi2Ab8zsR+fcqlMu5txLwEsAKSkp/nyKZqeOQ6jbITIc3uONE+QOhEM7vdcqxUOTbpB6nxcITVIgvp6/9UqZtHfvXhISEqhevTqbNm1i8uTJDBw4MKTv0bt3b959913OP/98fvzxR5YtW1bwSSFSUEC4M3wf7HFeGUDTXI8TgcxC1oVzLjPwdbWZfQt0BVble5JfklNh6Yewa43XopDSlZMN234+dexg20+c+F+0Xltoe2mgdXCu9zgq2teSpXzo1q0b7du3p2PHjrRo0YLU1NSQv8c999zDr371Kzp16kS3bt3o2LEjNWqUzgRLy6/5YmYHgZV4rYGWge8JPG7hnKuWz7kxwAqgP7ARmAf8wjm3NMixrwOfOefeDzyuBRx0zh0xs7rAd8CQPAPcp0hJSXFpaWn5/FXDaOtyeK4nDBnj7fsr4XVg+6ktg40L4Og+77UqtU4GQWKKt/9yBV7TqKxavnw57dq187uMiJCVlUVWVhZxcXGkp6czYMAA0tPTiYkp+j1GwT5XM5vvnEsJdnxB71Ds/0LOuSwzuxuYDEQDY51zS81sFJDmnJtoZucCHwG1gCvM7EHnXIfA+74YGLyOwhuDKL12VVHVawtV63jjEAqI0Mo66i1TkZF2MhR2rfFes2ho2BE6X38yFGq3UDeflCv79++nf//+ZGVl4ZzjxRdfLFY4FEe+7+KcW5f7sZnVAfoA651z8wu6uHPuc+DzPM/9Pdf38/C6nvKeNxsoO1OTzbw7XdbNKvhYObODO72uoa3Lva+bFnl3GWUf8V5PaOSFQMqtXuugUReoVNXfmkXCrGbNmsyfX+CP27Ao6DbXz4CRzrklZtYIWACkAS3N7CXn3OjSKLJMSO4NP30GezKgxmmZJ7kd3gNbf4Jty3N9XQ77t5w8plKCN+ms+3+cbB3UaOJfzSIVUEHtlObOucACM9wCTHHO/crMEoBZgALiuNzrMnW6zt9aIsWRfd7g8fEWwdZAEOzLda9CbFVv/kjL/t7ks/rtvC67GonqKhLxWUEBcSzX9/2BlwGcc/vK9OS2cGjQESrX8NZlqmgBcfSAFwS5u4e2Loc9uabBxMRB3bOgeR+o39bb07t+W28tKy2VLhKRCgqIDWZ2D94tq92ASQBmVgUo/LZEFUFUYPOXdbP9riR8jh3ylhTJ3T20dRnsXs+JW0qjK3lB0Kwn1Lv5ZIugVrJuLRUpYwoKiNuAUcBFwPXOud2B53sCBS9VWNEkp0L6ZNi3BRIa+F1N8WUdge3peVoEy2DXWnCBhmNULNRp5W232nW4FwL120Gt5hCtRYKlbOvbty9//vOfueSSS048N3r0aFasWMFzzz0X9Jz4+Hj2799PZmYm9957L++//37Q6z7++OOkpAS9q/TE+4wYMYKqVb0bMC699FLGjRtHzZqlf7t2QXcxbQXuCPL8VGBquIoqs46vy7R+trfSZ1n04/vw0R2QE+hdtGgvCBqeDWdfd7J7qE5LiFYjUsqnYcOGMWHChFMCYsKECTz22GMFntu4ceOg4VBYo0ePZvjw4ScC4vPPPy/gjPAp6C6mfJfZds4NDm05ZVyjzhBbzZsPURYD4sg+mPRn7+6hXvd6LYI6rSCmst+ViZSqoUOH8sADD3DkyBEqV67M2rVryczMpEuXLvTv359du3Zx7NgxHn74YYYMGXLKuWvXruXyyy9nyZIlHDp0iFtuuYVly5bRrl07Dh06dOK4O++8k3nz5nHo0CGGDh3Kgw8+yNNPP01mZiYXXnghdevWZerUqSQnJ5OWlkbdunV54oknGDt2LAC33347v/3tb1m7di2DBg2id+/ezJ49myZNmvDJJ59QpUqVEn8OBfUFnIe34N54YA4Fr79U5mRl5/Dp4kxSW9alfvW4kl0sOhaadi+74xCznoYDW2HYeG+egUgk+GIkbP4xtNdseDYMevSML9epU4fu3bszadIkhgwZwoQJE7j++uupUqUKH330EdWrV2f79u307NmTwYMHn3E7z+eff56qVauyePFiFi9efMpS3Y888gi1a9cmOzub/v37s3jxYu69916eeOIJpk6dSt26dU+51vz583nttdeYM2cOzjl69OjBBRdcQK1atUhPT2f8+PG8/PLLXHfddXzwwQcMH17ySbsF3T7SEPgL0BF4Cm9vh+3OuWnOuWklfvcIkLn7ML9/bzEvTV8dmgsmp3p7VB/cGZrrlZY9G2H2M9DxGoWDCCe7mcDrXho2bBjOOf7yl7/QqVMnLrroIjZu3MiWLVvOeI3p06ef+EHdqVMnOnXqdOK1d999l27dutG1a1eWLl1a4CJ8M2fO5KqrrqJatWrEx8dz9dVXn1g2vHnz5nTp4m2fk3up8JIqaAwiG+/OpUlmVhkYBnxrZqOcc8+EpAKfNatTlSu7NOGtOeu4o29L6saXsDvlxDjEd9D2spIXWFqmPgIuG/r/veBjRUpTPr/ph9OVV17J/fffz4IFCzh06BDdunXj9ddfZ9u2bcyfP5/Y2FiSk5ODLu+dW7DWxZo1a3j88ceZN28etWrV4uabby7wOvmtm3d8mXDwlgrP3ZVVEgXegG5mlc3sarx9Ie4CngY+DMm7R4i7LmzJ0awcXp4RglZEk3O8zWbWlqFlNzYthoXjoMcd3u2oIkJ8fDx9+/bl1ltvZdiwYYC3M1z9+vWJjY1l6tSprFu3Lt9r9OnTh7fffhuAJUuWsHjxYsBbJrxatWrUqFGDLVu28MUXX5w4JyEhgX379gW91scff8zBgwc5cOAAH330Eeeff36o/rpBFTRI/QZe99IXwIO5ZlWXKy3qxTO4c2Pe/G4dv+7TktrVSrBRTExlb1mIsrIuk3Pw5QPeiqfn/6ff1YhElGHDhnH11Vef6Gq68cYbueKKK0hJSaFLly60bds23/PvvPNObrnlFjp16kSXLl3o3r074O0M17VrVzp06HDaMuEjRoxg0KBBNGrUiKlTT94s2q1bN26++eYT17j99tvp2rVryLqTgiloue8c4EDgYe4DDXDOuephq6yISrrc98qt+7j4yen8pm9L/nBJ/v/RCzT1v2D6Y/CntRBXOuu2F9uKL2HctTDwv6HnaXc0i/hCy32HR1GX+863i8k5F+WcSwj8qZ7rT0IkhUMotKqfwGVnN+KN2evYffBoyS6W1MubULZhbmiKC5fsLJjyN2+JbG2XKiJ5aBGcXO7p15r9R7IYO2ttyS6U2B2iYrx1mSLZD296s6QvHqX9l0XkNAqIXNo0TGBQx4a8NmsNew4dK/iEM6lU1dvJLJLnQxzZ59251Ow8aHu539WISARSQORxd79W7DucxRuz15bsQsmpkLnAW+k0Es16Cg5sgwGPaFltiUj5jY9K0RXn81RA5NGhcQ0ubt+AV2euYd/hErQiklIhJ8vbIjPS7NkIs5+FjkMh8Ry/qxE5TVxcHDt27FBIhIhzjh07dhAXV7TVIrTsZhD39mvNFc/O5H+/W8ddF7Yq3kWa9gCL8uZDtOgbyvJK7puHNSlOIlpiYiIZGRls27bN71LKjbi4OBITi7bbpQIiiLMTa9CvbX1enrGam3olE1+5GB9TXHVo2CnyxiE2LYJF46HXPVArye9qRIKKjY2lefPmfpdR4amL6Qzu6deK3QeP8db3+c+UzFdyb6+L6Vj+U+hLzYlJcbU0KU5ECqSAOIOuzWrR56x6vDx9NQePZhXvIkm9IPuIN1gdCdKnwJrp0HekN3NaRCQfCoh83Ne/FTsOHGXcnPXFu0Cz87yvkbAuU3aW13qo3RLOucXvakSkDFBA5OOcpNqktqrDC9NWc/hYdtEvULU21O8QGesy/fC/sP1nuPhBTYoTkUJRQBTg3n6t2b7/COPnFrMVkZzqLbmRXYJbZkvqyD5vfahmvTQpTkQKTQFRgB4t6tCjeW1emLaqeK2IpF5w7IB395BfZo4OTIp7WJPiRKTQFBCFcF//1mzZe4T30jYU/eTjGwj5tS7Tno3wnSbFiUjRKSAK4byWdUhJqsVz367iSFYRWxHx9aFOa//mQ3zzsHd7qybFiUgRKSAKwcy476LWbNpzmA/mbyz6BZJTvS1Ic4rRRVUSxyfF9bxDk+JEpMgUEIXUu1VdujaryZipKzmWnVO0k5NS4che2FKKG/LlnhTX+/7Se18RKTcUEIVkZtzbvzUbdx/iowVFbEUk9fK+luZ8iPQvA5Pi/qxJcSJSLAqIIuh7Vj06Jdbg2akrySpKK6JGItRMKr35ENlZ8OXfvElxKZoUJyLFo4AoAjPj3n6tWb/zIJ8szCzaycm9vYHqnCJ2TxXHgjcCk+JGQXRs+N9PRMolBUQR9W9Xn/aNqvPs1JVk5xRhrfqkVDi00/vBHU6H98K3/wpMirssvO8lIuVaWAPCzAaa2c9mttLMRgZ5vY+ZLTCzLDMbmue1m8wsPfDnpnDWWbH0RO8AABA1SURBVBTHxyLWbD/Ap4uK0Io4MQ4R5vkQx3eKu0ST4kSkZMIWEGYWDYwBBgHtgWFm1j7PYeuBm4Fxec6tDfwD6AF0B/5hZrXCVWtRDWjfgLYNE3jmm/TCtyJqJUP1JuGdD3F8UtzZ10ITTYoTkZIJZwuiO7DSObfaOXcUmAAMyX2Ac26tc24xkLdj/hJginNup3NuFzAFGBjGWoskKsq4p19rVm07wOc/bircSWZeK2LdLO8W1HD45iFNihORkAlnQDQBcq9NkRF4LmTnmtkIM0szs7TS3ppwUMeGtK4fzzPfpJNT2FZEUirs3wI7V4e+oMyFsGgC9LwTajYL/fVFpMIJZ0AE6wAv7K/OhTrXOfeScy7FOZdSr169IhVXUlFRxt39WrFiy34mL91cuJPCtS7TKTvFaVKciIRGOAMiA2ia63EiUNhR3ZKcW2ou79SYFnWr8dTXhWxF1G0N1eqFfhxixWRYO8ObFBdXI7TXFpEKK5wBMQ9obWbNzawScAMwsZDnTgYGmFmtwOD0gMBzESU60Ir4afM+vlq+peATco9DhEp2Fkz5G9RppUlxIhJSYQsI51wWcDfeD/blwLvOuaVmNsrMBgOY2blmlgFcC7xoZksD5+4EHsILmXnAqMBzEWdw58Yk1anK09+k4woz+JzUG/ZsgN3F3IAorwVvwPYVmhQnIiEXE86LO+c+Bz7P89zfc30/D6/7KNi5Y4Gx4awvFGKio7jrwlb88f3FTP15K/3aNsj/hNzrMnUp4WDy8UlxSanQ5tKSXUtEJA/NpA6Bq7o2IbFWFZ76emXBrYj67SGuZmi6mWZppzgRCR8FRAjEBloRizbsZnr69vwPjooKzTjEngz4bgycfR006Vaya4mIBKGACJFruiXSuEYcT321ouBWRFKqNxdibyEn2QVzYqe4vxX/GiIi+VBAhEilmCjuvLAVC9bvZvaqHfkffHwcoritiMyFgZ3iNClORMJHARFC16Uk0rB6HE99nZ7/gQ07QaWE4gXE8UlxVetoUpyIhJUCIoQqx0RzxwUtmLtmJ9+vzqcVER0DzXoUb8KcJsWJSClRQITYDd2bUT+hMk8X1IpISoVtP8GBAga1c8s+FpgU1xrOublEdYqIFEQBEWJxsdH8+oKWzF61g3lr85nbd3xdpqK0IjQpTkRKkQIiDH7RvRl14yvl34po3BViqhR+HOLwXpj6L28mdptBoSlURCQfCogwqFIpmhF9WjAjfTvz1+0KflBMJWh6buEDYtZoOLgdBjykSXEiUioUEGFyY48kalerxDPf5NOKSOoNm5fAoTOEyHGaFCciPlBAhEm1yjHcfn5zvv15G4s27A5+UFIvwMH6Oflf7GvtFCcipU8BEUa/Oi+ZmlVjz9yKSEyB6EqwLp8NhDIXwuIJcN5voGbTMx8nIhJiCogwiq8cw22pzflq+VaWbNxz+gGxVaDJOWe+kyn3pLjevwtvsSIieSggwuym1GQS4mLO3IpISvVaCUf2nf7aikmaFCcivlFAhFn1uFhuTW3O5KVbWL5p7+kHJPUClw0b5p76fPYx+FKT4kTEPwqIUnBranPiK8fw7DcrT3+xaQ+w6NNvd13wBuxI16Q4EfGNAqIU1Kgay829kvl8ySZWbMnTlVQ5Hhp3OXUc4vikuOTzNSlORHyjgCglt/VuTpXY6OCtiKRU2Dgfjh3yHs98UpPiRMR3CohSUqtaJX51XjKfLs5k5db9p76YlArZRyEjDXZvgO+fg07Xe8txiIj4RAFRim4/vzlxMdE8NzVPK6JZT8C8cYhvHvKe66ed4kTEXwqIUlQ3vjLDezbj44UbWbv9wMkXqtSEhh1h4duw+B3oqUlxIuI/BUQp+48+LYiNjmJM3lZEUm/YvR6q1tWkOBGJCAqIUlY/IY5f9GjGhz9sZP2OgydfaH6+97XvSIir7k9xIiK5KCB8cMcFLYmOMp77Nlcr4qxBMPwDSLnNv8JERHJRQPigQfU4bji3Ke/PzyBjV6AVERUFrS7yvoqIRAD9NPLJHRe0xAye/3aV36WIiASlgPBJ45pVuDalKe+mbSBz9yG/yxEROY0Cwke/6dsS5+DFaWpFiEjkUUD4KLFWVYaek8j4eRvYsvew3+WIiJxCAeGz3/RtRXaO48Vpq/0uRUTkFAoInzWrU5Wrujbh7Tnr2LpPrQgRiRwKiAhw14WtOJadwysz1vhdiojICQqICNC8bjWGdGnCm9+t46fNQXadExHxgQIiQtzXvzVVK0Uz+JlZjJm6kqzsHL9LEpEKLqwBYWYDzexnM1tpZiODvF7ZzN4JvD7HzJIDzyeb2SEzWxj480I464wEyXWr8eXv+nBR+/o8Nvlnhr7w3en7RoiIlKKwBYSZRQNjgEFAe2CYmbXPc9htwC7nXCvgSeC/c722yjnXJfDnjnDVGUnqxFdmzC+68fSwrqzdcYDLnp7BKzNWk53j/C5NRCqgcLYgugMrnXOrnXNHgQnAkDzHDAHeCHz/PtDfrGLvsWlmDO7cmC9/14fzW9fl4f9bzg0vfce6HQcKPllEJITCGRBNgA25HmcEngt6jHMuC9gD1Am81tzMfjCzaWZ2frA3MLMRZpZmZmnbtm0LbfU+q58Qx8u/SuHxazvz0+Z9DBw9g//9bi05ak2ISCkJZ0AEawnk/el2pmM2Ac2cc12B+4FxZnbaJgnOuZeccynOuZR69eqVuOBIY2YMPSeRL3/Xh3Ob1+bvnyxl+KtzTq4AKyISRuEMiAwg976ZiUDmmY4xsxigBrDTOXfEObcDwDk3H1gFnBXGWiNaoxpVeOOWc/nX1WezaMNuBo6ewYS563FOrQkRCZ9wBsQ8oLWZNTezSsANwMQ8x0wEbgp8PxT4xjnnzKxeYJAbM2sBtAYq9FoUZsaw7s2Y9Ns+nN2kBiM//JFbXp/H5j2afS0i4RG2gAiMKdwNTAaWA+8655aa2SgzGxw47FWgjpmtxOtKOn4rbB9gsZktwhu8vsM5tzNctZYlTWtX5e3be/Dg4A7MWb2TAU9O48MFGWpNiEjIWXn5wZKSkuLS0tL8LqNUrd1+gN+/t4i0dbu4uH0DHrmqI/UT4vwuS0TKEDOb75xLCfaaZlKXYcl1q/HOr8/jr5e2Y9qKbVzy5HQ+XZR3mEdEpHgUEGVcdJTxH31a8Pm9vWlWuyr3jP+Bu95ewM4DR/0uTUTKOAVEOdGqfgIf3NmLP1zShi+XbWbAk9OYvHSz32WJSBmmgChHYqKjuOvCVnx6T28aVI/j12/O53fvLGTPwWN+lyYiZZACohxq27A6H9+Vyn39W/PpokwGjJ7G1J+2+l2WiJQxCohyKjY6it9dfBYf35VKzSqVuOX1efzx/UXsPazWhIgUjgKinOvYpAYT70nlN31b8v78DAY+OZ2Z6dv9LktEygAFRAVQOSaaPw5sywd39iKuUjTDX53DAx//yIEjWX6XJiIRTAFRgXRtVovP7z2f23s35+056xn01AzmrN7hd1kiEqEUEBVMXGw0D1zenndGnIcZ3PDy94z6dBmHjmb7XZqIRBgFRAXVvXltvrjvfH7ZM4mxs9Zw2dMzmL9ul99liUgEUUBUYFUrxTBqSEfevr0HR7JyuPaF2fzri+UcPqbWhIhosT4J2Hf4GI/833ImzNtAvYTK1KwS63dJIlJIbRtV55lhXYt1bn6L9cWUqCopNxLiYnn0mk4M7NiQ9+dnkFNOfnEQqQia1qoSlusqIOQUfdvUp2+b+n6XISIRQGMQIiISlAJCRESCUkCIiEhQCggREQlKASEiIkEpIEREJCgFhIiIBKWAEBGRoMrNUhtmtg1Y53cdJVQX0G4+J+nzOJU+j5P0WZyqJJ9HknOuXrAXyk1AlAdmlnamNVEqIn0ep9LncZI+i1OF6/NQF5OIiASlgBARkaAUEJHlJb8LiDD6PE6lz+MkfRanCsvnoTEIEREJSi0IEREJSgEhIiJBKSAigJk1NbOpZrbczJaa2X1+1+Q3M4s2sx/M7DO/a/GbmdU0s/fN7KfA/yPn+V2Tn8zsd4F/J0vMbLyZxfldU2kys7FmttXMluR6rraZTTGz9MDXWqF4LwVEZMgC/tM51w7oCdxlZu19rslv9wHL/S4iQjwFTHLOtQU6U4E/FzNrAtwLpDjnOgLRwA3+VlXqXgcG5nluJPC1c6418HXgcYkpICKAc26Tc25B4Pt9eD8AmvhblX/MLBG4DHjF71r8ZmbVgT7AqwDOuaPOud3+VuW7GKCKmcUAVYFMn+spVc656cDOPE8PAd4IfP8GcGUo3ksBEWHMLBnoCszxtxJfjQb+COT4XUgEaAFsA14LdLm9YmbV/C7KL865jcDjwHpgE7DHOfelv1VFhAbOuU3g/cIJhGRjeQVEBDGzeOAD4LfOub1+1+MHM7sc2Oqcm+93LREiBugGPO+c6wocIETdB2VRoG99CNAcaAxUM7Ph/lZVfikgIoSZxeKFw9vOuQ/9rsdHqcBgM1sLTAD6mdlb/pbkqwwgwzl3vEX5Pl5gVFQXAWucc9ucc8eAD4FePtcUCbaYWSOAwNetobioAiICmJnh9TEvd8494Xc9fnLO/dk5l+icS8YbfPzGOVdhf0N0zm0GNphZm8BT/YFlPpbkt/VATzOrGvh3058KPGify0TgpsD3NwGfhOKiMaG4iJRYKvBL4EczWxh47i/Ouc99rEkixz3A22ZWCVgN3OJzPb5xzs0xs/eBBXh3//1ABVt2w8zGA32BumaWAfwDeBR418xuwwvRa0PyXlpqQ0REglEXk4iIBKWAEBGRoBQQIiISlAJCRESCUkCIiEhQus1VpABmlg38CMTi3Vr5BjDaOaelQKRcU0CIFOyQc64LgJnVB8YBNfDuPy8RM4t2zmWX9Doi4aAuJpEicM5tBUYAd5sn2sweM7N5ZrbYzH4NYGZRZvZcYN+Cz8zsczMbGnhtrZn93cxmAteaWUszm2Rm881shpm1DRxXz8w+CFx7npml+vYXlwpJLQiRInLOrTazKLwVM4fgrSh6rplVBmaZ2ZfAOUAycHbguOXA2FyXOeyc6w1gZl8Ddzjn0s2sB/Ac0A9vH4gnnXMzzawZMBloVyp/SREUECLFZYGvA4BOx1sHeF1PrYHewHuBcYrNZjY1z/nvwIkVfHsB73lLCwFQOfD1IqB9ruerm1lCYM8QkbBTQIgUkZm1ALLxVsw04B7n3OQ8x1xWwGUOBL5GAbuPj3HkEQWc55w7VMKSRYpFYxAiRWBm9YAXgGedt5DZZODOwHLtmNlZgQ19ZgLXBMYiGuAtrnaawL4fa8zs2sD5ZmadAy9/Cdyd672DhYhI2KgFIVKwKoFVdo/f5vomcHxZ9lfwxhoWBJaf3oa33eMHeEtRLwFW4O0QuOcM178ReN7MHgi8xwRgEd7ey2PMbDHev9XpwB2h/suJnIlWcxUJEzOLd87tN7M6wFwgNbC/g0iZoBaESPh8ZmY1gUrAQwoHKWvUghARkaA0SC0iIkEpIEREJCgFhIiIBKWAEBGRoBQQIiIS1P8Ddec9cRg1exwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, max_deg+1), mse_tr, label='Training')\n",
    "plt.plot(range(1, max_deg+1), mse_val, label='Validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.059204210569073344"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_new = poly.fit_transform(Xtest)\n",
    "pred_test = regr.predict(Xtest_new)\n",
    "mean_squared_error(pred_test, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
